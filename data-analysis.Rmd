---
title: Traffic Data Analysis
output: html_document
---


### Loading data
#### The data is Traffic data that i crawled from bey2ollak.com. Reports from Feb 6 2016 to Feb 20 2016
```{r}
library(dplyr)
library(ggplot2)
df <- read.csv("all-semi-unique.csv", encoding = "UTF-8")

```

### Cleaning data
#### First we would like to remove some columns that might not be needed for the analysis. first start by removing advertisment related columns
```{r}
cols <- grep("ad/*",names(df),value=F)
df2 <- df[,-cols]
```

#### Also remove the rd.img column which has a value of true or NA.
```{r}
unique(df$rd.img)
df2 <- subset(df2,select=-c(rd.img))
```

#### Checking for columns with only one value and remove them as they won't contribute in a significant change.
```{r}
unique(df2$rd.rp.type)
df2 <- subset(df2,select=-c(rd.rp.type))
unique(df2$rd.cl)
df2 <- subset(df2,select=-c(rd.cl))
```


### Row duplicates
#### A second step would be removing some rows, first take a subset that excludes the crawling date time and report date time and road time. Since the data is crawled multiple of times some of the comments may be repeated but with a different crawl date, report date and road date so to determine duplicates we exclude them.
```{r}
df3 <- subset(df2,select=-c(rd.hr,rd.mn,rd.rp.hr,rd.rp.mn,crawl_date))
df3 <- df3[!duplicated(df3),]
```

#### However, knowing the reporting time may be useful for analysing the traffic status so add new columns for the dates of the road and comment which could be caluclated by considering "what time ago?" was the comment posted from the crawling time.
```{r}
x <- strptime(df2[,"crawl_date"],format = "%a %b %d %H:%M",tz = "UTC")
hrs <- function(u) {
 x <- u * 3600
 return(x)
}
mns <- function(m) {
 x <- m * 60
 return(x)
}
df2$tmp_date <- x
df2 <- mutate(df2, date= tmp_date-hrs(rd.hr)-mns(rd.mn))
df2 <- mutate(df2, rp.date= tmp_date-hrs(rd.rp.hr)-mns(rd.rp.mn))
df2 <- subset(df2,select=-c(rd.hr,rd.mn,rd.rp.hr,rd.rp.mn,crawl_date,tmp_date))
df2 <- df2[!duplicated(df2),]
```

#### To make sure that no comments are repeated check for further duplicates in cmid.
```{r}
length(unique(df2$rd.rp.cmid))
```

#### The length of df2 is still larger than the number of unique cmid, by observing the repeated cmids in df2, this is due to the difference in seconds as the crawl date contains seconds while the minused value is only hours and minutes, so accommodate for diffrences in seconds
```{r}
data <- subset(df2, !duplicated(df2[,'rd.rp.cmid']))
```

### Now begin observe and analyise the data!
#### Examine the frequency for each status for the comments 
```{r}
qplot(rd.rp.stid,data = data)
```

#### Examine rows with comment status that is NA
```{r}
sum(is.na(data$rd.rp.stid))
not.labeled <- data[is.na(data$rd.rp.stid),]
```

#### Examine the status  that corresponds to each status id
```{r}
t <- (data[ which(data$rd.rp.stid==1), ])
t <- (data[ which(data$rd.rp.stid==2), ])
t <- (data[ which(data$rd.rp.stid==3), ])
t <- (data[ which(data$rd.rp.stid==4), ])
t <- (data[ which(data$rd.rp.stid==5), ])
t <- (data[ which(data$rd.rp.stid==6), ])
t <- (data[ which(data$rd.rp.stid==7), ])
t <- (data[ which(data$rd.rp.stid==8), ])
t <- (data[ which(data$rd.rp.stid==10), ])
```

#### By observing, stid 1 relates to "7alawa", stid 2 to "lazeez", stid = 3 to "mashy", stid = 4 to "za7ma", stid = 5 to "mafeesh 2amal", stid = 6 to asking for updates like "eh el nezam?", stid = 7 to having a "lagna", stid = 8 to having 7adsa, stid = 9 to having a3tal, stid = 10 to the GPS records. 

#### Examine stid = NA and try to remove some of them.
```{r}
na.stid <- data[is.na(data$rd.rp.stid) == TRUE,]
```

#### Comments containing "clear" sometimes have NA value for stid. examine other values given to it.
```{r}
m <- data[grepl("(.)*clear(.)*", data$rd.rp.cm) == TRUE,]
m <- m[!is.na(m$rd.rp.stid),]
count(m[m$rd.rp.stid == 1,])
count(m[m$rd.rp.stid == 2,])
count(m[m$rd.rp.stid == 3,])
count(m[m$rd.rp.stid == 4,])
count(m[m$rd.rp.stid == 5,])
count(m[m$rd.rp.stid == 6,])
count(m[m$rd.rp.stid == 7,])
count(m[m$rd.rp.stid == 8,])
count(m[m$rd.rp.stid == 9,])
count(m[m$rd.rp.stid == 10,])
```

#### Replace stid that is equal to NA in "clear" comments with 1
```{r}
data$rd.rp.stid <- ifelse(grepl("(.)*clear(.)*", data$rd.rp.cm), 1,data$rd.rp.stid)
sum(is.na(data$rd.rp.stid))
not.labeled <- data[is.na(data$rd.rp.stid),]
data2 <- data[!is.na(data$rd.rp.stid),]
qplot(rd.rp.stid,data = data2)
```

#### Considering the main categories of comments that could be clearly calssified into positive and negative traffic (7alawa,lazeez,za7ma,mafeesh 2amal)
```{r}
data2 <- data2[data2$rd.rp.stid ==1 | data2$rd.rp.stid == 2 | data2$rd.rp.stid == 4 | data2$rd.rp.stid == 5, ]
```

#### Adding a column for positive comments (1,2)
```{r}
data2$pos.com <- ifelse(data2$rd.rp.stid == 1 | data2$rd.rp.stid == 2  , 1,0)
```

#### Adding a column for negative comments (4,5)
```{r}
data2$neg.com <- ifelse(data2$rd.rp.stid == 4 | data2$rd.rp.stid == 5 , 1,0)
```

### Examine roads with the most positive comments in general
```{r}
positive <- data2 %>% group_by(rd.ri) %>% summarise(sum.pos = sum(pos.com))
head(positive)
ggplot(positive, aes(positive$rd.ri,positive$sum.pos )) +   geom_point()+ scale_x_continuous(breaks = round(seq(min(positive$rd.ri), max(positive$rd.ri), by = 100),1))
positive[positive$sum.pos==max(positive$sum.pos),]
z <- data2[data2$rd.ri==553,]
qplot(rd.rp.stid,data = z)
```

#### Examine roads with the most negative comments in general
```{r}
negative <- data2 %>% group_by(rd.ri) %>% summarise(sum.neg = sum(neg.com))
head(negative)
ggplot(negative, aes(negative$rd.ri,negative$sum.neg )) +   geom_point()+ scale_x_continuous(breaks = round(seq(min(negative$rd.ri), max(negative$rd.ri), by = 100),1))
negative[negative$sum.neg==max(negative$sum.neg),]
zn <- data2[data2$rd.ri==166,]
qplot(rd.rp.stid,data = zn)
```


#### Examine the areas containg the roads
```{r}
data3 <- data2
data3$rd.nm <- as.character(data3$rd.nm)
temp <- as.data.frame(do.call(rbind, strsplit(data3$rd.nm, ";")))
data3 <- cbind(temp,data3)
positive.areas <- data3 %>% group_by(V1) %>% summarise(sum.pos = sum(pos.com))
positive.areas[positive.areas$sum.pos == max(positive.areas$sum.pos),]
negative.areas <- data3 %>% group_by(V1) %>% summarise(sum.neg = sum(neg.com))
negative.areas[negative.areas$sum.neg == max(negative.areas$sum.neg),]

```


#### Comparing positive comments to negative comments
```{r}
general <- cbind(positive.areas , negative.areas)
general$pos <- general$sum.pos / (general$sum.pos+general$sum.neg)
general$neg <- general$sum.neg / (general$sum.pos+general$sum.neg)
general$total <- general$sum.pos + general$sum.neg

pos <- general[general$pos > general$neg,]
neg <- general[general$pos < general$neg,]
slices <- neg$sum.neg
lbls <- neg$V1
pct <- round(slices/sum(neg$sum.neg)*100, digits = 2)
lbls <- paste(lbls, pct) 
lbls <- paste(lbls,"%",sep="") 
pie(slices,labels = lbls, col=rainbow(length(lbls)),
  	main="negative areas :( \n regarding number of negative comments")

```

#### Plot the percentage of negative comments for the negative areas
```{r}
ggplot(neg, aes(V1,(neg*100) )) +   geom_point() + theme(axis.text.x  = element_text(angle=90, vjust=0.5))

```

#### Number of reports for a specific hour ("UTC")
```{r}
data3$hour <- format(data3$rp.date, '%H')
m <- data3 %>% group_by(hour) %>% summarise(reports = length(rd.rp.stid))
ggplot(m, aes(hour, reports)) +   geom_point()
```

